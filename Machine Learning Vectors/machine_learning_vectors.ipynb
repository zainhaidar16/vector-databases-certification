{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models and Object Classification\n",
    "- Translation to Vectors: Imagine you have a library where books are organized by topics using a system like the Dewey Decimal System. Each book has a specific place based on its topic. Similarly, machine learning models translate data into vectors (a type of data representation) and place them in a \"vector space\" based on their meaning.\n",
    "\n",
    "- Machine Learning Models: These models, often neural networks, learn to identify and understand different data objects (like images or text). They translate similar objects into vectors that are close together and dissimilar objects into vectors that are farther apart. This helps retain the meaning of the data when it's converted into vectors.\n",
    "\n",
    "- Vector Databases: Vector databases use these AI-generated vectors to perform searches. For example, if you search for \"spicy food recipes,\" the database will find vectors (data points) that are close to this concept in the vector space, giving you relevant results.\n",
    "\n",
    "\n",
    "In essence, machine learning models help convert complex data into a format that computers can understand and work with efficiently, making searches faster and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating data from human to machine-understandable\n",
    "\n",
    "- Machine Learning Models: Think of these models as very smart programs that learn to recognize patterns. For example, they can look at millions of pictures of stop signs and learn what features make a stop sign (like its shape, color, and text).\n",
    "\n",
    "- Vectors: These are like special codes or numbers that represent data. When the model sees a stop sign, it converts the features it recognizes into a vector, a kind of unique fingerprint for that stop sign.\n",
    "\n",
    "- Text Understanding: For text, imagine filling in blanks in sentences. If you see \"The ___ howled at the moon,\" you know \"wolf\" fits. Models learn this by practicing with millions of sentences, understanding which words fit together.\n",
    "\n",
    "- Vector Databases: These databases store all these vectors. When you search for something, the database finds vectors (data points) that are similar to your search, giving you accurate results.\n",
    "\n",
    "\n",
    "In short, machine learning models turn complex data (like images and text) into simple numerical codes (vectors) that computers can easily understand and use for fast, accurate searches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML models and vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client is ready: True\n"
     ]
    }
   ],
   "source": [
    "# Connect to the locallay launched instance of Weaviate\n",
    "client = weaviate.Client(\"http://localhost:8080\")\n",
    "\n",
    "print(f\"Client is ready: {client.is_ready()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and delete already existed class\n",
    "current_schema = client.schema.get()['classes']\n",
    "\n",
    "for schema in current_schema:\n",
    "    if schema['class'] == 'ClipExample':\n",
    "        client.schema.delete_class(schema['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class created\n"
     ]
    }
   ],
   "source": [
    "# create a class object that use multi2vec module  \n",
    "class_object = {\n",
    "    \"class\": \"ClipExample\",\n",
    "    \"moduleConfig\": {\n",
    "        \"multi2vec-clip\": {\"imageFields\": [\"image\"]},\n",
    "    },\n",
    "    \"vectorizer\" : \"multi2vec-clip\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"text\",\n",
    "            \"dataType\": [\"string\"]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"image\",\n",
    "            \"dataType\": [\"blob\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.create_class(class_object)\n",
    "print(\"Class created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Big Sur_ California_LIL_9678.jpg added\n",
      "Image Bird feeder_LIL_134172.jpg added\n",
      "Image Blue sky between skyscrapers_LIL_134160.jpg added\n",
      "Image Cats on a chair_LIL_134151.jpg added\n",
      "Image Cat_LIL_134138.jpg added\n",
      "Image Cherry_LIL_134126.jpg added\n",
      "Image Cityscape_LIL_134155.jpg added\n",
      "Image Conservatory_LIL_9680.jpg added\n",
      "Image Deer_LIL_134180.jpg added\n",
      "Image Dog in motion_LIL_134175.jpg added\n",
      "Image Feet under a skirt_LIL_134201.jpg added\n",
      "Image Forest_LIL_134133.jpg added\n",
      "Image Golden Gate Bridge from Presidio_LIL_9682.jpg added\n",
      "Image Joshua Tree and California coast_LIL_9662.jpg added\n",
      "Image Joshua Tree and California coast_LIL_9670.jpg added\n",
      "Image Kitchen scene_LIL_134191.jpg added\n",
      "Image Llama_LIL_134178.jpg added\n",
      "Image Microphone_LIL_134215.jpg added\n",
      "Image On a grassy hill_LIL_134221.jpg added\n",
      "Image Onlookers contemplating_LIL_134225.jpg added\n",
      "Image Ornate furniture_LIL_134195.jpg added\n",
      "Image Pies and dishes_LIL_134217.jpg added\n",
      "Image Point Reyes_ California_LIL_9672.jpg added\n",
      "Image Reading outside_LIL_134129.jpg added\n",
      "Image Single cyclist with land background_LIL_9376.dng added\n",
      "Image Speaker_LIL_134226.jpg added\n",
      "Image Spiral staircase_LIL_134156.jpg added\n",
      "Image Stack of books_LIL_134182.jpg added\n",
      "Image Table setting outdoor_LIL_134165.jpg added\n",
      "Image Walking dog on beach_LIL_134163.jpg added\n",
      "Image Wearing a white dress _LIL_134222.jpg added\n",
      "All images processed\n"
     ]
    }
   ],
   "source": [
    "# Add images to our created class\n",
    "image_folder = r\"D:/Vector Databases/vector-databases-certification/Machine Learning Vectors/Images\"\n",
    "\n",
    "for imgs in os.listdir(image_folder):\n",
    "    encoded_image = weaviate.util.image_encoder_b64(f\"{image_folder}/{imgs}\")\n",
    "    data_properties = {\n",
    "        \"text\": imgs,\n",
    "        \"image\": encoded_image\n",
    "    }\n",
    "    try:\n",
    "        client.data_object.create(data_properties, \"ClipExample\")\n",
    "        print(f\"Image {imgs} added\")\n",
    "    except weaviate.exceptions.UnexpectedStatusCodeException as e:\n",
    "        print(f\"Failed to add image {imgs}: {e}\")\n",
    "\n",
    "print(\"All images processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
